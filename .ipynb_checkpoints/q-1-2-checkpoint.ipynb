{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Train the decision tree with categorical and numerical features. Report precision, recall, f1 score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import sys #for maxint, minint\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, root, pos, neg, tf, split):\n",
    "        self.root = root\n",
    "        self.pos = pos #no of yes\n",
    "        self.neg = neg #no of no\n",
    "        self.children = {} #subtrees\n",
    "        self.isLeaf = tf #whether a leaf not or not\n",
    "        self.split = split #where to split for numerical\n",
    "        if(pos>neg):\n",
    "            self.result=1 #decision here\n",
    "        else:\n",
    "            self.result=0\n",
    "    def add_child(self, key, val):\n",
    "        self.children[key]=val\n",
    "\n",
    "def countYesNo(dataframe): #counts no of positives and negatives at a node\n",
    "    left_col = dataframe['left']\n",
    "    pos = dataframe[left_col == 1] \n",
    "    \n",
    "    #no of rows of yes\n",
    "    yes_r = pos.shape[0]\n",
    "\n",
    "    neg = dataframe[left_col == 0]\n",
    "    \n",
    "    #no of rows of no\n",
    "    no_r = neg.shape[0]\n",
    "    \n",
    "    return yes_r, no_r\n",
    "\n",
    "#build Tree using entropy\n",
    "#param: dataframe, numerical, categorical lists\n",
    "#return: root of the tree- tree_root\n",
    "def buildTree(dataframe, num, categ): \n",
    "    yes, no = countYesNo(dataframe)\n",
    "    \n",
    "    if no==0: #node is yes->1\n",
    "        return Node(1,yes,0,True,-1)\n",
    "        \n",
    "    elif yes==0: #node is no->0\n",
    "        return Node(0,0,no,True,-1)\n",
    "    \n",
    "    root_node,split = findMaxInfoGain(dataframe, num, categ) #work_acc\n",
    "    \n",
    "    tree_root = Node(root_node, yes, no, False,split)\n",
    "    \n",
    "    \n",
    "    root_col = dataframe[root_node] #work_acc col\n",
    "    \n",
    "    if root_node in num:\n",
    "        #numerical attribute\n",
    "        grea_than = dataframe[dataframe[root_node] > split]\n",
    "        less_than = dataframe[dataframe[root_node] <= split]\n",
    "        \n",
    "        less_than_tree = buildTree(less_than, num, categ)\n",
    "        grea_than_tree = buildTree(grea_than, num, categ)\n",
    "        \n",
    "        lname = \"less_than_\"+(str(split))\n",
    "        gname = \"greater_than\"+str(split)\n",
    "        tree_root.add_child(lname,less_than_tree)\n",
    "        tree_root.add_child(gname,grea_than_tree)\n",
    "        \n",
    "    \n",
    "    elif root_node in categ:\n",
    "        s = pd.Series(root_col) \n",
    "        unique_val = s.unique() \n",
    "        for i in unique_val:\n",
    "            array = dataframe[root_col == i]\n",
    "\n",
    "            #dataframe for current unique value\n",
    "            curr = pd.DataFrame(array)\n",
    "\n",
    "            #now drop this col\n",
    "            curr = curr.drop(root_node , 1)\n",
    "\n",
    "            recursive_root = buildTree(curr, num, categ)\n",
    "\n",
    "            tree_root.add_child(i,recursive_root)\n",
    "    \n",
    "    return tree_root\n",
    "\n",
    "\n",
    "#Step-1: Compute impurity score of training label distribution\n",
    "#entropy of entire dataset, params: dataframe, target- target col\n",
    "#returns entropy \n",
    "def entropyCalculate(dataframe, target):\n",
    "    total_len=len(dataframe)\n",
    "    col=dataframe[target]\n",
    "    s = pd.Series(col) \n",
    "    count_arr=s.value_counts() #has no of 0s and 1s\n",
    "    entropy=0\n",
    "    for counts in count_arr:\n",
    "        prob = float(counts)/total_len\n",
    "        entropy += -1*prob* np.log2(prob)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "#Step-2: Compute impurity score for each unique value of candidate attributes\n",
    "#Step-3: Compute impurity score for candidate attribute \n",
    "#entropy for each attribute, params: dataframe, coloumnName, target\n",
    "#returns entropy for sub attributes\n",
    "def entropyAttribute(dataframe, col_name, label):\n",
    "    col=dataframe[col_name] \n",
    "    \n",
    "    #total rows in this col, eg sales -8990\n",
    "    col_len=len(col) \n",
    "    \n",
    "    #convert the column to series\n",
    "    s = pd.Series(col)\n",
    "    \n",
    "    #find all unique attr, eg low,med,high\n",
    "    unique_val = s.unique() \n",
    "\n",
    "    total_entropy=0\n",
    "    \n",
    "    for i in unique_val:\n",
    "        #all rows in the col where sub attribute is i, eg for sales , i=accounting..\n",
    "        ar = dataframe[col==i]\n",
    "        \n",
    "        #find no. of rows for that subattribute\n",
    "        total_r = ar.shape[0]\n",
    "        \n",
    "        #made a dataframe for this attribute\n",
    "        cur_df = pd.DataFrame(ar) \n",
    "        \n",
    "        #now suppose attribute=sales, value=accounting, find entropy now for accounting\n",
    "        curr_entropy = entropyCalculate(cur_df, label)\n",
    "        \n",
    "        fraction = float(total_r)/col_len\n",
    "        total_entropy += fraction * curr_entropy\n",
    "        \n",
    "    return total_entropy    \n",
    "\n",
    "#finds split for numerical column, calculates entire entropy and finds the minimum one\n",
    "#param: dataframe, numerical col\n",
    "#return: minimum entropy and corresponding split point\n",
    "def findSplit(dataframe, col):\n",
    "    label = 'left'\n",
    "    min = sys.maxint\n",
    "    idx=0\n",
    "    for j in pd.Series(dataframe[col]).unique():\n",
    "        less_than = dataframe[dataframe[col] > j]\n",
    "        grea_than = dataframe[dataframe[col] <= j]\n",
    "\n",
    "        less_rows = less_than.shape[0]\n",
    "        grea_rows = grea_than.shape[0]\n",
    "        tot = less_rows + grea_rows\n",
    "        e1 = entropyCalculate(less_than, label)\n",
    "        e2 = entropyCalculate(grea_than, label)\n",
    "\n",
    "        entropy = ((e1*less_rows)/tot + (e2*grea_rows)/tot)\n",
    "        if(min>entropy):\n",
    "            min = entropy\n",
    "            idx = j\n",
    "    return min, idx\n",
    "\n",
    "#a helper traverse function to visualize the tree\n",
    "def traverse(root):\n",
    "    if len(root.children)==0:\n",
    "        print \"return root: \",root.root\n",
    "        return\n",
    "    \n",
    "    print \"Root: \",root.root\n",
    "    \n",
    "    for k,v in root.children.items():\n",
    "        print \"root: \",root.root, \"key: \",k\n",
    "        traverse(v)\n",
    "# traverse(root)\n",
    "\n",
    "#prediction function\n",
    "def predict2(row,root,num,categ,default=0):\n",
    "    \n",
    "    if(root.isLeaf == True):\n",
    "        return root.result\n",
    "   \n",
    "    col=root.root\n",
    "    split_at = root.split\n",
    "    val=row[col]\n",
    "    less_key = ''\n",
    "    grea_key = ''\n",
    "    if col in num:\n",
    "        for k,v in root.children.items():\n",
    "            if k[0]=='l':\n",
    "                less_key = k\n",
    "            else:\n",
    "                grea_key = k\n",
    "            \n",
    "        if val > split_at:\n",
    "            return predict2(row, root.children[grea_key], num,categ)\n",
    "        else:\n",
    "            return predict2(row, root.children[less_key], num,categ)\n",
    "\n",
    "    elif col in categ:\n",
    "        if val in root.children.keys():\n",
    "            return predict2(row,root.children[val],num,categ)\n",
    "        else:\n",
    "            return root.result\n",
    "\n",
    "def helper(root, predict_col, df_sample):\n",
    "    df_sample[predict_col] = df_sample.apply(predict2, axis=1, args=(root,numerical,categorical,0))\n",
    "    return df_sample[predict_col]\n",
    "    \n",
    "#pred_label = predict(model,model_args,X) \n",
    "#where model = decision tree object, model_args = parameters to be passed, X = test sample.\n",
    "\n",
    "def predict(model,model_args,X):\n",
    "    df_sample = pd.read_csv(X)\n",
    "    left_col = helper(model,model_args, df_sample)\n",
    "    return left_col\n",
    "\n",
    "\n",
    "\n",
    "#a helper function for making predictions, adds a new col of name predict_col to store the prediction    \n",
    "def helper_validate(df,root, predict_col):\n",
    "    df[predict_col] = df.apply(predict2, axis=1, args=(root,numerical,categorical,0))\n",
    "    a,p,f,r = findMeasures(df, predict_col)\n",
    "    \n",
    "\n",
    "#finds measures for tp, fp, tn, fn , accuracy,precision, recall\n",
    "def findMeasures(df, predict_col):\n",
    "    truePos=0\n",
    "    trueNeg=0\n",
    "    falsePos=0\n",
    "    falseNeg=0\n",
    " \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "#         print \"index, predict, left,\",index, row[\"prediction\"], row[\"left\"]\n",
    "        if row[predict_col]==0 and row[\"left\"]==0:\n",
    "            trueNeg += 1\n",
    "            \n",
    "        elif row[predict_col]==0 and row[\"left\"]==1:\n",
    "            falseNeg += 1\n",
    "        \n",
    "        elif row[predict_col]==1 and row[\"left\"]==1:\n",
    "            truePos += 1\n",
    "        elif row[predict_col]==1 and row[\"left\"]==0:\n",
    "            falsePos += 1\n",
    "#     print \"TP, TN, FP, FN: \", truePos, trueNeg, falsePos, falseNeg\n",
    "    sumtotal = truePos + trueNeg + falsePos + falseNeg\n",
    "    accuracy = ((float)(truePos + trueNeg))/sumtotal\n",
    "    precision = ((float)(truePos))/(truePos + falsePos)\n",
    "    recall = ((float)(truePos))/(truePos + falseNeg)\n",
    "    try:\n",
    "        f1_score_den = 1.0/recall + 1.0/precision\n",
    "        f1_score = 2.0/f1_score_den\n",
    "    except:\n",
    "        f1_score=0\n",
    "    print \"A, P, R, F: \",accuracy*100, precision*100, recall*100, f1_score\n",
    "    return accuracy*100, precision*100, recall*100, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-4: Compute Information Gain (reduction in impurity score) provided by candidate attribute\n",
    "#Step-5: Compare Information Gain provided by all candidates\n",
    "#params: dataframe, numerical, categorical lists\n",
    "#returns root and splitPoint (if numerical)\n",
    "\n",
    "def findMaxInfoGain(dataframe, num, categ):\n",
    "    unique_cols = dataframe.columns.tolist()\n",
    "    label = 'left'\n",
    "    \n",
    "    entropy = entropyCalculate(dataframe, label)\n",
    "    max = -sys.maxint - 1\n",
    "    temp = 0\n",
    "    attr_entropy=0\n",
    "    split=0\n",
    "    for i in unique_cols:\n",
    "        if i != label:            \n",
    "            if i in num:\n",
    "                #this is a numerical attribute\n",
    "                attr_entropy, idx = findSplit(dataframe, i)\n",
    "                \n",
    "            elif i in categ:\n",
    "                attr_entropy = entropyAttribute(dataframe, i, label)\n",
    "            \n",
    "            temp = entropy - attr_entropy\n",
    "            \n",
    "            if temp>max:\n",
    "                max=temp\n",
    "                root=i\n",
    "                if i in num:\n",
    "                    split = idx\n",
    "                else:\n",
    "                    split = -1\n",
    "    return root, split        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=raw_input('Enter filename: ')\n",
    "filename = \"train.csv\"\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "dataset = dataset.sample(frac=1)\n",
    "train, validate = np.split(dataset, [int(.8*len(dataset))])\n",
    "\n",
    "numerical = ['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company']\n",
    "categorical = ['Work_accident','promotion_last_5years','sales','salary','left']\n",
    "root = buildTree(train, numerical, categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy:\n",
      "0    0\n",
      "1    1\n",
      "Name: left, dtype: int64\n",
      "A, P, R, F:  97.5088967972 92.8846153846 96.2151394422 0.945205479452\n"
     ]
    }
   ],
   "source": [
    "testFile = \"sample_test.csv\"\n",
    "pred_label = predict(root, 'left', testFile )\n",
    "print \"Entropy:\"\n",
    "print pred_label\n",
    "helper_validate(validate, root, 'prediction_ent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
