{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-5: Plot a graph of training and validation error with respect todepth of your decision tree.  Also plot the training and validation error with respect to number of nodes in the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#for plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "import sys #for maxint, minint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, root, pos, neg, tf, split):\n",
    "        self.root = root\n",
    "        self.pos = pos #no of yes\n",
    "        self.neg = neg #no of no\n",
    "        self.children = {} #subtrees\n",
    "        self.isLeaf = tf #whether a leaf not or not\n",
    "        self.split = split #where to split for numerical\n",
    "        if(pos>neg):\n",
    "            self.result=1 #decision here\n",
    "        else:\n",
    "            self.result=0\n",
    "    def add_child(self, key, val):\n",
    "        self.children[key]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countYesNo(dataframe): #counts no of positives and negatives at a node\n",
    "    left_col = dataframe['left']\n",
    "    pos = dataframe[left_col == 1] \n",
    "    \n",
    "    #no of rows of yes\n",
    "    yes_r = pos.shape[0]\n",
    "\n",
    "    neg = dataframe[left_col == 0]\n",
    "    \n",
    "    #no of rows of no\n",
    "    no_r = neg.shape[0]\n",
    "    \n",
    "    return yes_r, no_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-1: Compute impurity score of training label distribution\n",
    "#entropy of entire dataset, params: dataframe, target- target col\n",
    "#returns entropy \n",
    "def entropyCalculate(dataframe, target):\n",
    "    total_len=len(dataframe)\n",
    "    col=dataframe[target]\n",
    "    s = pd.Series(col) \n",
    "    count_arr=s.value_counts() #has no of 0s and 1s\n",
    "    entropy=0\n",
    "    for counts in count_arr:\n",
    "        prob = float(counts)/total_len\n",
    "        entropy += -1*prob* np.log2(prob)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-2: Compute impurity score for each unique value of candidate attributes\n",
    "#Step-3: Compute impurity score for candidate attribute \n",
    "#entropy for each attribute, params: dataframe, coloumnName, target\n",
    "#returns entropy for sub attributes\n",
    "def entropyAttribute(dataframe, col_name, label):\n",
    "    col=dataframe[col_name] \n",
    "    \n",
    "    #total rows in this col, eg sales -8990\n",
    "    col_len=len(col) \n",
    "    \n",
    "    #convert the column to series\n",
    "    s = pd.Series(col)\n",
    "    \n",
    "    #find all unique attr, eg low,med,high\n",
    "    unique_val = s.unique() \n",
    "\n",
    "    total_entropy=0\n",
    "    \n",
    "    for i in unique_val:\n",
    "        #all rows in the col where sub attribute is i, eg for sales , i=accounting..\n",
    "        ar = dataframe[col==i]\n",
    "        \n",
    "        #find no. of rows for that subattribute\n",
    "        total_r = ar.shape[0]\n",
    "        \n",
    "        #made a dataframe for this attribute\n",
    "        cur_df = pd.DataFrame(ar) \n",
    "        \n",
    "        #now suppose attribute=sales, value=accounting, find entropy now for accounting\n",
    "        curr_entropy = entropyCalculate(cur_df, label)\n",
    "        \n",
    "        fraction = float(total_r)/col_len\n",
    "        total_entropy += fraction * curr_entropy\n",
    "        \n",
    "    return total_entropy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step-4: Compute Information Gain (reduction in impurity score) provided by candidate attribute\n",
    "#Step-5: Compare Information Gain provided by all candidates\n",
    "#params: dataframe, numerical, categorical lists\n",
    "#returns root and splitPoint (if numerical)\n",
    "\n",
    "def findMaxInfoGain(dataframe, num, categ):\n",
    "    unique_cols = dataframe.columns.tolist()\n",
    "    label = 'left'\n",
    "    \n",
    "    entropy = entropyCalculate(dataframe, label)\n",
    "    max = -sys.maxint - 1\n",
    "    temp = 0\n",
    "    attr_entropy=0\n",
    "    split=0\n",
    "    for i in unique_cols:\n",
    "        if i != label:            \n",
    "            if i in num:\n",
    "                #this is a numerical attribute\n",
    "                attr_entropy, idx = findSplit(dataframe, i)\n",
    "                \n",
    "            elif i in categ:\n",
    "                attr_entropy = entropyAttribute(dataframe, i, label)\n",
    "            \n",
    "            temp = entropy - attr_entropy\n",
    "            \n",
    "            if temp>max:\n",
    "                max=temp\n",
    "                root=i\n",
    "                if i in num:\n",
    "                    split = idx\n",
    "                else:\n",
    "                    split = -1\n",
    "    return root, split        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds split for numerical column, calculates entire entropy and finds the minimum one\n",
    "#param: dataframe, numerical col\n",
    "#return: minimum entropy and corresponding split point\n",
    "def findSplit(dataframe, col):\n",
    "    label = 'left'\n",
    "    min = sys.maxint\n",
    "    idx=0\n",
    "    for j in pd.Series(dataframe[col]).unique():\n",
    "        less_than = dataframe[dataframe[col] > j]\n",
    "        grea_than = dataframe[dataframe[col] <= j]\n",
    "\n",
    "        less_rows = less_than.shape[0]\n",
    "        grea_rows = grea_than.shape[0]\n",
    "        tot = less_rows + grea_rows\n",
    "        e1 = entropyCalculate(less_than, label)\n",
    "        e2 = entropyCalculate(grea_than, label)\n",
    "\n",
    "        entropy = ((e1*less_rows)/tot + (e2*grea_rows)/tot)\n",
    "        if(min>entropy):\n",
    "            min = entropy\n",
    "            idx = j\n",
    "    return min, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build Tree using entropy\n",
    "#param: dataframe, numerical, categorical lists\n",
    "#return: root of the tree- tree_root\n",
    "def buildTree(dataframe, num, categ): \n",
    "    yes, no = countYesNo(dataframe)\n",
    "    \n",
    "    if no==0: #node is yes->1\n",
    "        return Node(1,yes,0,True,-1)\n",
    "        \n",
    "    elif yes==0: #node is no->0\n",
    "        return Node(0,0,no,True,-1)\n",
    "    \n",
    "    root_node,split = findMaxInfoGain(dataframe, num, categ) #work_acc\n",
    "    \n",
    "    tree_root = Node(root_node, yes, no, False,split)\n",
    "    \n",
    "    root_col = dataframe[root_node] #work_acc col\n",
    "    \n",
    "    if root_node in num:\n",
    "        #numerical attribute\n",
    "        grea_than = dataframe[dataframe[root_node] > split]\n",
    "        less_than = dataframe[dataframe[root_node] <= split]\n",
    "        \n",
    "        less_than_tree = buildTree(less_than, num, categ)\n",
    "        grea_than_tree = buildTree(grea_than, num, categ)\n",
    "        \n",
    "        lname = \"less_than_\"+(str(split))\n",
    "        gname = \"greater_than\"+str(split)\n",
    "        tree_root.add_child(lname,less_than_tree)\n",
    "        tree_root.add_child(gname,grea_than_tree)\n",
    "        \n",
    "    \n",
    "    elif root_node in categ:\n",
    "        s = pd.Series(root_col) \n",
    "        unique_val = s.unique() \n",
    "        for i in unique_val:\n",
    "            array = dataframe[root_col == i]\n",
    "\n",
    "            #dataframe for current unique value\n",
    "            curr = pd.DataFrame(array)\n",
    "\n",
    "            #now drop this col\n",
    "            curr = curr.drop(root_node , 1)\n",
    "\n",
    "            recursive_root = buildTree(curr, num, categ)\n",
    "\n",
    "            tree_root.add_child(i,recursive_root)\n",
    "    \n",
    "    return tree_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a helper traverse function to visualize the tree\n",
    "def traverse(root):\n",
    "    if len(root.children)==0:\n",
    "        print \"return root: \",root.root\n",
    "        return\n",
    "    \n",
    "    print \"Root: \",root.root\n",
    "    \n",
    "    for k,v in root.children.items():\n",
    "        print \"root: \",root.root, \"key: \",k\n",
    "        traverse(v)\n",
    "# traverse(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds measures for tp, fp, tn, fn , accuracy,precision, recall\n",
    "def findMeasures(df, predict_col):\n",
    "    truePos=0\n",
    "    trueNeg=0\n",
    "    falsePos=0\n",
    "    falseNeg=0\n",
    " \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row[predict_col]==0 and row[\"left\"]==0:\n",
    "            trueNeg += 1\n",
    "            \n",
    "        elif row[predict_col]==0 and row[\"left\"]==1:\n",
    "            falseNeg += 1\n",
    "        \n",
    "        elif row[predict_col]==1 and row[\"left\"]==1:\n",
    "            truePos += 1\n",
    "        elif row[predict_col]==1 and row[\"left\"]==0:\n",
    "            falsePos += 1\n",
    "#     print \"TP, TN, FP, FN: \", truePos, trueNeg, falsePos, falseNeg\n",
    "    sumtotal = truePos + trueNeg + falsePos + falseNeg\n",
    "    accuracy = ((float)(truePos + trueNeg))/sumtotal\n",
    "   \n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def predict(row,root,num,categ,default=0):\n",
    "    \n",
    "    if(root.isLeaf == True):\n",
    "        return root.result\n",
    "   \n",
    "    col=root.root\n",
    "    split_at = root.split\n",
    "    val=row[col]\n",
    "    less_key = ''\n",
    "    grea_key = ''\n",
    "    if col in num:\n",
    "        for k,v in root.children.items():\n",
    "            if k[0]=='l':\n",
    "                less_key = k\n",
    "            else:\n",
    "                grea_key = k\n",
    "            \n",
    "        if val > split_at:\n",
    "            return predict(row, root.children[grea_key], num,categ)\n",
    "        else:\n",
    "            return predict(row, root.children[less_key], num,categ)\n",
    "\n",
    "    elif col in categ:\n",
    "        if val in root.children.keys():\n",
    "            return predict(row,root.children[val],num,categ)\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "#a helper function for making predictions, adds a new col of name predict_col to store the prediction    \n",
    "def helper(df,root, predict_col):\n",
    "    df[predict_col] = df.apply(predict, axis=1, args=(root,numerical,categorical,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNodesDepth(root, depth):\n",
    "#     print \"root: \",root.root, \" depth: \",depth\n",
    "    num_nodes = 0\n",
    "    if(root.isLeaf or depth==1 ):\n",
    "        return 1\n",
    "   \n",
    "    for k in root.children.keys():\n",
    "        num_nodes += countNodesDepth(root.children[k], depth-1)\n",
    "    return num_nodes+1\n",
    "\n",
    "def forEachNodeCount(root, depth):\n",
    "    node_count = []\n",
    "    for i in range (1, depth+1):\n",
    "        d = countNodesDepth(root,i)\n",
    "        node_count.append(d)\n",
    "    return node_count\n",
    "\n",
    "def predictDepth(row,root,depth,num,categ,default=0):\n",
    "    \n",
    "    if(root.isLeaf == True or depth == 0):\n",
    "        return root.result   \n",
    "    col=root.root\n",
    "    split_at = root.split\n",
    "    val=row[col]\n",
    "    less_key = ''\n",
    "    grea_key = ''\n",
    "    if col in num:\n",
    "        for k,v in root.children.items():\n",
    "            if k[0]=='l': #less than\n",
    "                less_key = k\n",
    "            else:\n",
    "                grea_key = k\n",
    "            \n",
    "        if val > split_at:\n",
    "            return predictDepth(row, root.children[grea_key],depth-1, num,categ)\n",
    "        else:\n",
    "            return predictDepth(row, root.children[less_key], depth-1,num,categ)\n",
    "\n",
    "    elif col in categ:\n",
    "        if val in root.children.keys():\n",
    "            return predictDepth(row,root.children[val],depth-1,num,categ)\n",
    "        else:\n",
    "            return root.result\n",
    "    \n",
    "def helperDepth(df,root,depth,predict_col):\n",
    "    df[predict_col] = df.apply(predictDepth, axis=1, args=(root,depth,numerical,categorical,0))\n",
    "\n",
    "def findDepth(root):\n",
    "    depth = 0\n",
    "    max_depth = 0\n",
    "    if(root.isLeaf):\n",
    "        return 1\n",
    "\n",
    "    for k in root.children.keys():\n",
    "        depth = findDepth(root.children[k])\n",
    "        \n",
    "        if(depth + 1 > max_depth):\n",
    "            max_depth = 1 + depth\n",
    "            \n",
    "    return max_depth\n",
    "\n",
    "def forEachDepth(df, root, col_name):\n",
    "    max_depth = findDepth(root) #finds max depth of tree\n",
    "    error_ar_depth = [] #stores error at each depth\n",
    "    for i in range(1, max_depth + 1):     \n",
    "        helperDepth(df,root,i,col_name)\n",
    "        error = 100 - findMeasures(df, col_name)\n",
    "        error_ar_depth.append(error)    \n",
    "    \n",
    "    return error_ar_depth, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraphs():\n",
    "    error_depth_x, depth, y_node, error_depth_train = solveEntropy(train, validate, numerical, categorical)\n",
    "\n",
    "    fig, axes = plt.subplots(1,2,figsize=(10, 10))\n",
    "    y_depth = range(1,depth+1)\n",
    "    print \"error depth x: \", error_depth_x\n",
    "    print \"error depth train: \", error_depth_train\n",
    "    print \"y depth: \", y_depth\n",
    "    print \"y node: \", y_node\n",
    "    axes[0].plot(error_depth_x, y_depth, label=\"validation error\")\n",
    "    axes[0].plot(error_depth_train, y_depth, label=\"training error\")\n",
    "    axes[1].plot(error_depth_x,y_node, label=\"validation error\")\n",
    "    axes[1].plot(error_depth_train,y_node, label=\"training error\")\n",
    "\n",
    "    axes[0].set_title('Error vs Depth')\n",
    "    axes[0].set_xlabel('Error')\n",
    "    axes[0].set_ylabel('Depth')\n",
    "    axes[0].legend(loc='best')\n",
    "    axes[1].set_xlabel('Error')\n",
    "    axes[1].set_ylabel('Nodes')\n",
    "    axes[1].legend(loc='best')\n",
    "    \n",
    "    axes[1].set_title('Error vs Nodes')\n",
    "    fig.savefig(\"q5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveEntropy(df_t, df_v, num, cat):\n",
    "    root = buildTree(df_t, num, cat)\n",
    "    helper(df_v,root,'prediction') #prediction for entire\n",
    "\n",
    "    error_ar_depth, depth = forEachDepth(df_v,root,'predict_validate')\n",
    "    error_depth_train = forEachDepth(df_t,root,'predict_train')[0]\n",
    "    nodes = forEachNodeCount(root, depth)\n",
    "    \n",
    "    return error_ar_depth, depth, nodes, error_depth_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filename=raw_input('Enter filename: ')\n",
    "filename = \"../input_data/train.csv\"\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "train, validate = np.split(dataset, [int(.8*len(dataset))])\n",
    "\n",
    "numerical = ['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company']\n",
    "categorical = ['Work_accident','promotion_last_5years','sales','salary','left']\n",
    "plotGraphs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
